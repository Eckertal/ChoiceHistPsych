{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we create the datafile for the logistic regression model with one history predictor. \n",
    "\n",
    "(c) Anna-Lena Eckert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import pandas as pd\n",
    "import operator\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects N:  51\n",
      "Test passed, all datafiles intact. \n"
     ]
    }
   ],
   "source": [
    "# find and read datafiles. \n",
    "def find_datafiles(path): \n",
    "    os.chdir(path)\n",
    "    files = glob.glob('*.csv')\n",
    "    return files\n",
    "\n",
    "files = find_datafiles('C:\\\\Users\\\\annae\\\\Dropbox\\\\PhD\\\\RDK\\\\all_data\\\\all')\n",
    "\n",
    "print('Subjects N: ', len(files))\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    sbj_name = file.split('.')[0]\n",
    "    df['sbj_id'] = sbj_name\n",
    "    \n",
    "    # one case where csv is written with ; instad of ,...\n",
    "    if df.shape[1] == 1: \n",
    "        df = pd.read_csv(file, delimiter=';')\n",
    "        df['sbj_id'] = sbj_name\n",
    "    \n",
    "    # make sure incomplete datafiles are not added into final dataframe. \n",
    "    if df.shape == (768, 16): \n",
    "        dfs.append(df)\n",
    "    \n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "all_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# make sure no broken datafiles are in there. \n",
    "assert all_df.shape[0]/len(dfs) == 96*8\n",
    "print('Test passed, all datafiles intact. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need all sorts of transformations to our datafile in order to get started with the regression models. \n",
    "# motion direction: 0 (right) and 1 (left) instead of 0 and 180. \n",
    "df = all_df.copy()\n",
    "df['motionDirection'].replace({180: 1}, inplace=True)\n",
    "df['response'].replace({180: 1}, inplace=True)\n",
    "\n",
    "# coherence: 1-6 instead of float numbers, increasing coh = stronger evidence\n",
    "df['coherence'].replace({0.0005: 1, 0.0162: 2, 0.0315: 3, 0.0792: 4, 0.1991: 5, 0.5: 6}, inplace=True)\n",
    "\n",
    "# block coding was reversed compared to auditory task, now 0= rep, 1=neut\n",
    "df['block_type'].replace({0: 1, 1:0}, inplace=True)\n",
    "\n",
    "# rename into target column\n",
    "df = df.rename(columns={'motionDirection':'target'})\n",
    "\n",
    "# create cue column, where 0= right, 1=left\n",
    "df['cue'] = np.nan\n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    if row['cueValid'] == 1: \n",
    "        cue = row['target']\n",
    "        \n",
    "    elif row['cueValid'] == 0: \n",
    "        if row['target'] == 0: \n",
    "            cue = 1 # left\n",
    "        elif row['target'] == 1: \n",
    "            cue = 0 #right\n",
    "            \n",
    "    df.at[index, 'cue'] = cue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history column - some functions\n",
    "def make_nan_col(df, name): \n",
    "    df[name] = np.nan\n",
    "\n",
    "# collect previous k response and make them NaN at beginning of blocks\n",
    "def collect_history(df, k, columnName, target_col):\n",
    "     # iterate over df\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # find previous response and transform to int type\n",
    "        prev = df[df.index==index-k][columnName].values.astype(int)\n",
    "        \n",
    "        # missed responses? and the beginning of blocks should also be at zero. \n",
    "        if prev.shape == (0,):\n",
    "            prev = np.nan\n",
    "            \n",
    "        elif prev.shape == (): \n",
    "            prev = np.nan\n",
    "        \n",
    "        # if block counter incremented (i.e. new block started), it should also be nan. \n",
    "        elif df.at[index, 'Block'] != df.at[index-1, 'Block']: \n",
    "            prev = np.nan \n",
    "        \n",
    "        # otherwise use previous response...\n",
    "        else: \n",
    "            prev = prev[0]\n",
    "            \n",
    "          \n",
    "        # ...and write it into target column. \n",
    "        #target_col = str('resp_%i'%k)\n",
    "        df.at[index, target_col] = prev\n",
    "\n",
    "def make_nans_history(df, k):\n",
    "    \n",
    "    target_col = 'resp_%i'%k\n",
    "    resps = [0.0, 1.0]\n",
    "    \n",
    "    indexes = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        if df.at[index, target_col] not in resps:\n",
    "            indexes.append(index)\n",
    "            \n",
    "    for i in indexes: \n",
    "    \n",
    "        if i > 7:\n",
    "            \n",
    "            for j in range(1,k): \n",
    "\n",
    "                df.at[i+j, target_col] = np.nan\n",
    "              \n",
    "    return indexes\n",
    "\n",
    "def make_nans_stim_hists(df, k):\n",
    "    \n",
    "    target_col = 'stim_%i'%k\n",
    "    resps = [0.0, 1.0]\n",
    "    \n",
    "    indexes = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        if df.at[index, target_col] not in resps:\n",
    "            indexes.append(index)\n",
    "            \n",
    "    for i in indexes: \n",
    "    \n",
    "        if i > 7:\n",
    "            \n",
    "            for j in range(1,k): \n",
    "\n",
    "                df.at[i+j, target_col] = np.nan\n",
    "              \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make choice history column with previous t-1 response\n",
    "# change here to create datafiles for supplementary analyses\n",
    "# hists = ['resp_%i'%i for i in range(1,8)]\n",
    "\n",
    "hists = ['resp_1']\n",
    "\n",
    "for name in hists: \n",
    "    make_nan_col(df, name)\n",
    "    \n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for k in range(1,2): \n",
    "    collect_history(df, k, 'response', 'resp_%i'%k)\n",
    "    \n",
    "# make stimulus history column with previous stimulus. \n",
    "# to check supplementary analyses, uncomment the list comprehension statement \n",
    "# stim_hists = ['stim_%i' % i for i in range(1,8)]\n",
    "\n",
    "stim_hists = ['stim_1']\n",
    "\n",
    "for name in stim_hists[0:1]: \n",
    "    make_nan_col(df, name)\n",
    "    \n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# collect stimulus history ( this part may take long )\n",
    "for k in range(1,2): \n",
    "    collect_history(df, k, 'target', 'stim_%i'%k)\n",
    "    \n",
    "# make the first i responses at block beginning = nan\n",
    "for k in range(1,2):\n",
    "    make_nans_history(df,k)\n",
    "    \n",
    "for k in range(1,2): \n",
    "    make_nans_stim_hists(df,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.085106382978723"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read PPS data. \n",
    "# read CAPS and PDI scores. \n",
    "os.chdir('C:\\\\Users\\\\annae\\\\Dropbox\\\\PhD\\\\RDK\\\\all_data\\\\pps\\\\all')\n",
    "onlyfiles = [f for f in os.listdir(os.getcwd()) if os.path.isfile(os.path.join(os.getcwd(),f))]\n",
    "caps_data = [f for f in onlyfiles if f[6:] == '_caps.csv']\n",
    "pdi_data = [f for f in onlyfiles if f[6:] == '_pdi.csv']\n",
    "\n",
    "# exclude the dude that only filled out caps\n",
    "# caps_data.remove('rga09b_caps.csv')\n",
    "\n",
    "def read_caps(file, sbj_caps): \n",
    "    sbj_name = file.split('_')[0]\n",
    "    \n",
    "    columns = ['agree', 'distress', 'distract', 'freq']\n",
    "    caps = pd.read_csv(file,header=None,index_col=0)\n",
    "    caps.columns = columns\n",
    "    \n",
    "    caps_sum = caps.loc['Global'].agree\n",
    "    caps_distress = caps.loc['Global'].distress\n",
    "    caps_distract = caps.loc['Global'].distract\n",
    "    caps_freq = caps.loc['Global'].freq\n",
    "    \n",
    "    sbj_caps[sbj_name] = caps_sum\n",
    "    \n",
    "    return\n",
    "\n",
    "sbj_caps = dict()\n",
    "for file in caps_data:    \n",
    "    read_caps(file, sbj_caps)\n",
    "np.mean(list(sbj_caps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.326086956521739"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_pdi(file, sbj_caps): \n",
    "    sbj_name = file.split('_')[0]\n",
    "    \n",
    "    columns = ['agree', 'distress', 'distract', 'freq']\n",
    "    pdi = pd.read_csv(file,header=None,index_col=0)\n",
    "    pdi.columns = columns\n",
    "    \n",
    "    pdi_sum = pdi.loc['Global'].agree\n",
    "    pdi_distress = pdi.loc['Global'].distress\n",
    "    pdi_distract = pdi.loc['Global'].distract\n",
    "    pdi_freq = pdi.loc['Global'].freq\n",
    "    \n",
    "    sbj_caps[sbj_name] = pdi_sum\n",
    "    \n",
    "    return\n",
    "\n",
    "sbj_pdi = dict()\n",
    "for file in pdi_data:    \n",
    "    read_pdi(file, sbj_pdi)\n",
    "np.mean(list(sbj_pdi.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scz_list = [sbj_caps, sbj_pdi] \n",
    "scz = pd.DataFrame.from_dict(scz_list)\n",
    "scz.index=['caps','pdi']\n",
    "scz = scz.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental data N:  46 | Survey data N:  46\n",
      "No survey data for subjects:  ['yhy01m', 'tgw10b', 'yni11l']\n"
     ]
    }
   ],
   "source": [
    "# delete from df all of those who did not complete scz instruments\n",
    "scz = scz.dropna() # 1 person has a NaN value for CAPS - he was fed up and didnt want to fill it. \n",
    "sbj_exp = list(df['sbj_id'].unique())\n",
    "sbj_scz = list(scz.index)\n",
    "print('Experimental data N: ',len(sbj_exp), '| Survey data N: ',len(sbj_scz))\n",
    "\n",
    "no_scz = list(set(sbj_exp).difference(set(sbj_scz)))\n",
    "print('No survey data for subjects: ', no_scz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in no_scz: \n",
    "    df.drop(df[df.sbj_id == subject].index, inplace=True)\n",
    "    \n",
    "assert df.shape[0]/ len(list(df['sbj_id'].unique())) == 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caps'] = np.nan\n",
    "df['pdi'] = np.nan\n",
    "subjects = list(df['sbj_id'].unique())\n",
    "scz = scz.T\n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    sbj_name = row['sbj_id']\n",
    "    \n",
    "    if sbj_name in subjects: \n",
    "        caps_score = scz[scz.index=='caps'][sbj_name][0]\n",
    "        pdi_score = scz[scz.index=='pdi'][sbj_name][0]\n",
    "        \n",
    "        df.at[index, 'caps'] = int(caps_score)\n",
    "        df.at[index, 'pdi'] = int(pdi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add PPS score, sum of z-trans pdi and caps\n",
    "pps_measures = ['pdi', 'caps']\n",
    "\n",
    "for measure in pps_measures: \n",
    "    col_zscore = measure + '_zscore'\n",
    "    df[col_zscore] = (df[measure] - df[measure].mean())/df[measure].std(ddof=0)\n",
    "\n",
    "df['PPS'] = df['pdi_zscore'] + df['caps_zscore']\n",
    "\n",
    "# z-standardise columns\n",
    "cols = list(df.columns)\n",
    "[cols.remove(item) for item in ['sbj_id', 'Trial', 'Block', 'onset_tone', 'onset_rdk', 'end_rdk', 'onset_responseWin', 'key_press', 'pdi_zscore', 'caps_zscore']]\n",
    "\n",
    "for col in cols: \n",
    "    col_z = col + '_z'\n",
    "    df[col_z] = (df[col]- df[col].mean())/df[col].std(ddof=0)\n",
    "    \n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underperformers: \n",
      "ANI05H 0.4644736842105263\n",
      "HUT15A 0.5631578947368421\n",
      "Total included: 41\n"
     ]
    }
   ],
   "source": [
    "# exclude based on performance criteria from our pre-reg!\n",
    "\n",
    "subjects = list(df['sbj_id'].unique())\n",
    "underperformers = []\n",
    "\n",
    "performance = {}\n",
    "\n",
    "print('Underperformers: ')\n",
    "\n",
    "for sbj in subjects: \n",
    "    \n",
    "    df_sbj = df[df['sbj_id']==sbj]\n",
    "    \n",
    "    corr = df_sbj.correct.replace(99, 0).sum()\n",
    "    \n",
    "    perf = corr / df_sbj.shape[0]\n",
    "    if perf > 0.9: \n",
    "        print(sbj, perf)\n",
    "    elif perf < 0.6: \n",
    "        print(sbj, perf)\n",
    "        underperformers.append(sbj)\n",
    "        \n",
    "    performance[sbj] = perf\n",
    "\n",
    "\n",
    "for sbj in underperformers: \n",
    "    df.drop(df[df.sbj_id == sbj].index, inplace=True)\n",
    "    \n",
    "print('Total included: %i'%len(list(df.sbj_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6552631578947369"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.pop('ANI05H',None)\n",
    "performance.pop('HUT15A',None)\n",
    "\n",
    "performance[min(performance.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final csv file into correct repo\n",
    "os.chdir('C:\\\\Users\\\\annae\\\\Dropbox\\\\PhD\\\\RDK\\\\all_data\\\\')\n",
    "df.to_csv('model1_visual_final.csv')\n",
    "\n",
    "df_neut = df[df['block_type']==1]\n",
    "df_neut.to_csv('model1_visual_neutOnly.csv')\n",
    "\n",
    "df_rep = df[df['block_type']==0]\n",
    "df_rep.to_csv('model1_visual_repOnly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for PMF, we need signed stimulus intensities. \n",
    "# signed stimulus intensity. \n",
    "for index, row in df.iterrows(): \n",
    "    \n",
    "    if row['target'] == 0: \n",
    "        coh = -row['coherence']/10\n",
    "    elif row['target'] == 1: \n",
    "        coh = row['coherence']/10\n",
    "    \n",
    "    df.at[index, 'coherence'] = coh\n",
    "    \n",
    "df.to_csv('visual_pmf_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
